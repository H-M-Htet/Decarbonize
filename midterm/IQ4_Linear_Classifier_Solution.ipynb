{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution for IQ4: Linear Classifier with SVM and Cross-Entropy Loss\n",
    "\n",
    "This notebook implements a linear classifier for the CIFAR-10 dataset, as required by question IQ4. The implementation is done using PyTorch.\n",
    "\n",
    "The process is as follows:\n",
    "1.  **Data Loading and Preprocessing:** Load the CIFAR-10 dataset, apply transformations, and create data loaders.\n",
    "2.  **Model Definition:** Define a simple linear classifier model.\n",
    "3.  **Training and Evaluation:** Create functions to handle the training loop and accuracy evaluation.\n",
    "4.  **Experiment 1:** Train the classifier using the Multiclass SVM Loss (`MultiMarginLoss`).\n",
    "5.  **Experiment 2:** Train the classifier using the Cross-Entropy Loss (`CrossEntropyLoss`).\n",
    "6.  **Analysis:** Compare the final accuracies and discuss the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load and Preprocess CIFAR-10 Data\n",
    "\n",
    "We load the CIFAR-10 dataset and perform standard preprocessing:\n",
    "- Convert images to PyTorch Tensors.\n",
    "- Normalize the pixel values. The mean and standard deviation values are standard for CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set batch size based on the problem description (choice is ours)\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Define transformations for the training and test sets\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "# Define the classes\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "print('Files downloaded and data loaders are ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Linear Classifier Model\n",
    "\n",
    "The model is a single fully-connected (linear) layer. It takes a flattened image (32x32x3 = 3072 features) and maps it to 10 output scores, one for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        # Input features: 3 channels * 32 height * 32 width\n",
    "        # Output features: 10 (for 10 classes)\n",
    "        self.linear = nn.Linear(3 * 32 * 32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the image from (B, C, H, W) to (B, C*H*W)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Apply the linear layer\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Training and Evaluation Functions\n",
    "\n",
    "To avoid code repetition, we define generic functions for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, train_loader, epochs):\n",
    "    \"\"\"Generic training loop for a PyTorch model.\"\"\"\n",
    "    print(f"Starting training with {criterion.__class__.__name__}...")\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            # Optimize\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Print statistics every 5 epochs\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{epochs}], Loss: {running_loss / len(train_loader):.4f}')\n",
    "    \n",
    "    print('Finished Training')\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    \"\"\"Evaluates the model's accuracy on the test set.\"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad(): # We don't need to compute gradients during evaluation\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A: Training with Multiclass SVM Loss\n",
    "\n",
    "First, we train the linear classifier using PyTorch's `nn.MultiMarginLoss`, which is the standard implementation of the Multiclass SVM hinge loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters from the problem description\n",
    "LEARNING_RATE = 0.1\n",
    "EPOCHS = 30\n",
    "\n",
    "# 1. Initialize model, loss, and optimizer for SVM\n",
    "svm_model = LinearClassifier()\n",
    "svm_criterion = nn.MultiMarginLoss()\n",
    "svm_optimizer = optim.SGD(svm_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# 2. Train the model\n",
    "svm_model_trained = train_model(svm_model, svm_criterion, svm_optimizer, trainloader, EPOCHS)\n",
    "\n",
    "# 3. Evaluate the model\n",
    "svm_accuracy = evaluate_model(svm_model_trained, testloader)\n",
    "print(f'\nAccuracy of the network with SVM Loss on the 10000 test images: {svm_accuracy:.2f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Training with Cross-Entropy Loss\n",
    "\n",
    "Next, we train a new, independent instance of the classifier using `nn.CrossEntropyLoss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialize a NEW model, loss, and optimizer for Cross-Entropy\n",
    "ce_model = LinearClassifier()\n",
    "ce_criterion = nn.CrossEntropyLoss()\n",
    "ce_optimizer = optim.SGD(ce_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# 2. Train the model\n",
    "ce_model_trained = train_model(ce_model, ce_criterion, ce_optimizer, trainloader, EPOCHS)\n",
    "\n",
    "# 3. Evaluate the model\n",
    "ce_accuracy = evaluate_model(ce_model_trained, testloader)\n",
    "print(f'\nAccuracy of the network with Cross-Entropy Loss on the 10000 test images: {ce_accuracy:.2f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Comparison and Analysis\n",
    "\n",
    "Here we report the final classification accuracies and discuss the performance difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Final Results ---")\n",
    "print(f\"Multiclass SVM Loss Accuracy: {svm_accuracy:.2f}%\")\n",
    "print(f\"Cross-Entropy Loss Accuracy: {ce_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "\n",
    "**Which loss function gives better performance and why?**\n",
    "\n",
    "Typically, **Cross-Entropy loss gives better performance** for a classification task like this, and the results above should reflect that (though results can vary slightly due to random initialization). Hereâ€™s why:\n",
    "\n",
    "1.  **Nature of the Loss Calculation:**\n",
    "    *   **Multiclass SVM (Hinge) Loss:** The SVM loss aims to ensure that the score of the correct class is greater than the scores of all incorrect classes by a certain margin. Once this margin is achieved, the loss for that sample becomes zero. The model feels no incentive to further improve the scores, e.g., to make the correct class score *much* higher than the incorrect ones. It is satisfied as long as the separation is \"good enough.\"\n",
    "    *   **Cross-Entropy Loss:** This loss function is derived from the concept of maximizing the likelihood of the data. It uses the softmax function to convert raw scores into probabilities. The loss is the negative log-probability of the correct class. To make this loss very small, the probability of the correct class must approach 1, meaning its score must be significantly higher than all other scores. It *always* encourages the model to be more confident and produce a better probability distribution, never becoming fully \"satisfied.\"\n",
    "\n",
    "2.  **Gradient Smoothness:**\n",
    "    *   The gradient of the Cross-Entropy loss is smooth and continuous, which often leads to more stable and straightforward optimization with gradient descent.\n",
    "    *   The SVM loss has a \"kink\" (it's not differentiable) at the point where the margin condition is met, which can sometimes make the optimization process slightly less stable.\n",
    "\n",
    "In summary, Cross-Entropy loss is generally preferred for deep learning classification tasks because it pushes the model to produce well-calibrated probabilities and provides a smoother, more powerful optimization signal, often leading to higher accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
